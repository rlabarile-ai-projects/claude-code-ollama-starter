# ─────────────────────────────────────────────
# Claude Code + Ollama - Local Environment Config
# Copy this file to .env and adjust as needed
# ─────────────────────────────────────────────

# Ollama server base URL (default: local)
OLLAMA_BASE_URL=http://localhost:11434

# ── Model selection ──────────────────────────
# Pick ONE of the recommended coding models below
# and set it as OLLAMA_MODEL.
#
# Recommended options (pull before use):
#   qwen3-coder        → Best overall for coding (Alibaba, 7B–32B variants)
#   gpt-oss            → OpenAI open-source drop-in, strong reasoning
#   glm-4-9b           → GLM-5 family, fast and capable
#   deepseek-coder-v2  → Excellent for backend / algorithmic tasks
#   codellama:34b      → Meta's classic, solid for Java/Python
#
OLLAMA_MODEL=qwen3-coder

# ── Claude Code env wiring ───────────────────
# These are read by Claude Code automatically.
# Do NOT change the variable names.
ANTHROPIC_BASE_URL=${OLLAMA_BASE_URL}
ANTHROPIC_AUTH_TOKEN=ollama
ANTHROPIC_API_KEY=

# ── Optional: Ollama model pull on setup ─────
# Set to "true" to auto-pull OLLAMA_MODEL during setup.sh
AUTO_PULL_MODEL=true

# ── Context window ───────────────────────────
# Claude Code needs at least 64k tokens.
# Most qwen3-coder / glm variants support 128k.
# Adjust only if your model requires it.
CLAUDE_CODE_MAX_TOKENS=65536
